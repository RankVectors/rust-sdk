/*
 * RankVectors API
 *
 * Intelligent internal linking optimization API using AI.   RankVectors helps you automatically discover and implement optimal internal links  across your website to improve SEO performance and user experience.  ## Key Features - **AI-Powered Analysis**: Uses OpenAI embeddings to find optimal linking opportunities - **Smart Crawling**: Automatically crawls and analyzes your website content - **Automated Implementation**: Implement links via webhooks or manual instructions - **Credit-Based System**: Pay-per-use model with transparent pricing - **Multi-Platform Support**: Works with any CMS or platform via REST API  ## Getting Started 1. Create a project with your website URL 2. Start a crawl to analyze your content 3. Generate AI-powered link suggestions 4. Implement suggestions via API or webhook 5. Track performance and manage credits  ## Authentication All API endpoints require authentication using your RankVectors API key. Include your API key in the `Authorization` header: ``` Authorization: Bearer YOUR_API_KEY ```  Get your API key from your RankVectors dashboard: Settings â†’ API Keys 
 *
 * The version of the OpenAPI document: 1.2.0
 * Contact: support@rankvectors.com
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct Project {
    /// Unique project identifier
    #[serde(rename = "id")]
    pub id: String,
    /// Project name
    #[serde(rename = "name")]
    pub name: String,
    /// Website domain URL
    #[serde(rename = "domain")]
    pub domain: String,
    /// User who owns the project
    #[serde(rename = "userId")]
    pub user_id: String,
    /// Natural language prompt for crawling
    #[serde(rename = "prompt", skip_serializing_if = "Option::is_none")]
    pub prompt: Option<String>,
    /// Search query for targeted crawling
    #[serde(rename = "searchQuery", skip_serializing_if = "Option::is_none")]
    pub search_query: Option<String>,
    /// How to handle sitemaps
    #[serde(rename = "sitemapMode", skip_serializing_if = "Option::is_none")]
    pub sitemap_mode: Option<SitemapMode>,
    /// Whether to include subdomains
    #[serde(rename = "includeSubdomains", skip_serializing_if = "Option::is_none")]
    pub include_subdomains: Option<bool>,
    /// Whether to ignore URL query parameters
    #[serde(rename = "ignoreQueryParams", skip_serializing_if = "Option::is_none")]
    pub ignore_query_params: Option<bool>,
    /// Maximum crawl depth
    #[serde(rename = "maxDiscoveryDepth", skip_serializing_if = "Option::is_none")]
    pub max_discovery_depth: Option<i32>,
    /// Paths to exclude from crawling
    #[serde(rename = "excludePaths", skip_serializing_if = "Option::is_none")]
    pub exclude_paths: Option<Vec<String>>,
    /// Specific paths to include
    #[serde(rename = "includePaths", skip_serializing_if = "Option::is_none")]
    pub include_paths: Option<Vec<String>>,
    /// Whether to crawl the entire domain
    #[serde(rename = "crawlEntireDomain", skip_serializing_if = "Option::is_none")]
    pub crawl_entire_domain: Option<bool>,
    /// Whether to allow external links
    #[serde(rename = "allowExternalLinks", skip_serializing_if = "Option::is_none")]
    pub allow_external_links: Option<bool>,
    /// Maximum number of pages to crawl
    #[serde(rename = "maxPages", skip_serializing_if = "Option::is_none")]
    pub max_pages: Option<i32>,
    /// Delay between crawl requests (ms)
    #[serde(rename = "crawlDelay", skip_serializing_if = "Option::is_none")]
    pub crawl_delay: Option<i32>,
    /// Maximum concurrent crawl requests
    #[serde(rename = "crawlMaxConcurrency", skip_serializing_if = "Option::is_none")]
    pub crawl_max_concurrency: Option<i32>,
    /// Whether to extract only main content
    #[serde(rename = "onlyMainContent", skip_serializing_if = "Option::is_none")]
    pub only_main_content: Option<bool>,
    /// Custom headers for crawling
    #[serde(rename = "customHeaders", skip_serializing_if = "Option::is_none")]
    pub custom_headers: Option<std::collections::HashMap<String, String>>,
    /// Wait time for page load (ms)
    #[serde(rename = "waitFor", skip_serializing_if = "Option::is_none")]
    pub wait_for: Option<i32>,
    /// Whether to block ads
    #[serde(rename = "blockAds", skip_serializing_if = "Option::is_none")]
    pub block_ads: Option<bool>,
    /// Proxy mode for crawling
    #[serde(rename = "proxyMode", skip_serializing_if = "Option::is_none")]
    pub proxy_mode: Option<ProxyMode>,
    /// Whether to use AI reranking
    #[serde(rename = "useReranking", skip_serializing_if = "Option::is_none")]
    pub use_reranking: Option<bool>,
    /// Whether to enable change tracking
    #[serde(rename = "enableChangeTracking", skip_serializing_if = "Option::is_none")]
    pub enable_change_tracking: Option<bool>,
    /// Project creation timestamp
    #[serde(rename = "createdAt")]
    pub created_at: String,
    /// Last update timestamp
    #[serde(rename = "updatedAt")]
    pub updated_at: String,
    #[serde(rename = "_count", skip_serializing_if = "Option::is_none")]
    pub _count: Option<Box<models::ProjectCount>>,
}

impl Project {
    pub fn new(id: String, name: String, domain: String, user_id: String, created_at: String, updated_at: String) -> Project {
        Project {
            id,
            name,
            domain,
            user_id,
            prompt: None,
            search_query: None,
            sitemap_mode: None,
            include_subdomains: None,
            ignore_query_params: None,
            max_discovery_depth: None,
            exclude_paths: None,
            include_paths: None,
            crawl_entire_domain: None,
            allow_external_links: None,
            max_pages: None,
            crawl_delay: None,
            crawl_max_concurrency: None,
            only_main_content: None,
            custom_headers: None,
            wait_for: None,
            block_ads: None,
            proxy_mode: None,
            use_reranking: None,
            enable_change_tracking: None,
            created_at,
            updated_at,
            _count: None,
        }
    }
}
/// How to handle sitemaps
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum SitemapMode {
    #[serde(rename = "include")]
    Include,
    #[serde(rename = "exclude")]
    Exclude,
    #[serde(rename = "only")]
    Only,
}

impl Default for SitemapMode {
    fn default() -> SitemapMode {
        Self::Include
    }
}
/// Proxy mode for crawling
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum ProxyMode {
    #[serde(rename = "auto")]
    Auto,
    #[serde(rename = "residential")]
    Residential,
    #[serde(rename = "datacenter")]
    Datacenter,
}

impl Default for ProxyMode {
    fn default() -> ProxyMode {
        Self::Auto
    }
}

