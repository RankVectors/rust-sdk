/*
 * RankVectors API
 *
 * Intelligent internal linking optimization API using AI.   RankVectors helps you automatically discover and implement optimal internal links  across your website to improve SEO performance and user experience.  ## Key Features - **AI-Powered Analysis**: Uses OpenAI embeddings to find optimal linking opportunities - **Smart Crawling**: Automatically crawls and analyzes your website content - **Automated Implementation**: Implement links via webhooks or manual instructions - **Page-Based Plans**: Predictable pricing by number of pages monitored - **Multi-Platform Support**: Works with any CMS or platform via REST API  ## Getting Started 1. Create a project with your website URL 2. Start a crawl to analyze your content 3. Generate AI-powered link suggestions 4. Implement suggestions via API or webhook 5. Track performance and manage page usage and limits  ## Authentication Most API endpoints support authentication using your RankVectors API key. Include your API key in the `Authorization` header: ``` Authorization: Bearer YOUR_API_KEY ```  Get your API key from your RankVectors dashboard: Settings â†’ API Keys  **Note**: Some endpoints (marked in the documentation) support both API key authentication and web session authentication (Stack Auth).  API key authentication is required for SDK usage and external integrations like WordPress plugins. 
 *
 * The version of the OpenAPI document: 1.3.1
 * Contact: tj@rankvectors.com
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct Project {
    /// Unique project identifier
    #[serde(rename = "id")]
    pub id: String,
    /// Project name
    #[serde(rename = "name")]
    pub name: String,
    /// Website domain URL
    #[serde(rename = "domain")]
    pub domain: String,
    /// User who owns the project
    #[serde(rename = "userId")]
    pub user_id: String,
    /// Natural language prompt for crawling
    #[serde(rename = "prompt", skip_serializing_if = "Option::is_none")]
    pub prompt: Option<String>,
    /// Search query for targeted crawling
    #[serde(rename = "searchQuery", skip_serializing_if = "Option::is_none")]
    pub search_query: Option<String>,
    /// How to handle sitemaps
    #[serde(rename = "sitemapMode", skip_serializing_if = "Option::is_none")]
    pub sitemap_mode: Option<SitemapMode>,
    /// Whether to include subdomains
    #[serde(rename = "includeSubdomains", skip_serializing_if = "Option::is_none")]
    pub include_subdomains: Option<bool>,
    /// Whether to ignore URL query parameters
    #[serde(rename = "ignoreQueryParams", skip_serializing_if = "Option::is_none")]
    pub ignore_query_params: Option<bool>,
    /// Maximum crawl depth
    #[serde(rename = "maxDiscoveryDepth", skip_serializing_if = "Option::is_none")]
    pub max_discovery_depth: Option<i32>,
    /// Paths to exclude from crawling
    #[serde(rename = "excludePaths", skip_serializing_if = "Option::is_none")]
    pub exclude_paths: Option<Vec<String>>,
    /// Specific paths to include
    #[serde(rename = "includePaths", skip_serializing_if = "Option::is_none")]
    pub include_paths: Option<Vec<String>>,
    /// Whether to crawl the entire domain
    #[serde(rename = "crawlEntireDomain", skip_serializing_if = "Option::is_none")]
    pub crawl_entire_domain: Option<bool>,
    /// Whether to allow external links
    #[serde(rename = "allowExternalLinks", skip_serializing_if = "Option::is_none")]
    pub allow_external_links: Option<bool>,
    /// Maximum number of pages to crawl
    #[serde(rename = "maxPages", skip_serializing_if = "Option::is_none")]
    pub max_pages: Option<i32>,
    /// Delay between crawl requests (ms)
    #[serde(rename = "crawlDelay", skip_serializing_if = "Option::is_none")]
    pub crawl_delay: Option<i32>,
    /// Maximum concurrent crawl requests
    #[serde(rename = "crawlMaxConcurrency", skip_serializing_if = "Option::is_none")]
    pub crawl_max_concurrency: Option<i32>,
    /// Whether to extract only main content
    #[serde(rename = "onlyMainContent", skip_serializing_if = "Option::is_none")]
    pub only_main_content: Option<bool>,
    /// Custom headers for crawling
    #[serde(rename = "customHeaders", skip_serializing_if = "Option::is_none")]
    pub custom_headers: Option<std::collections::HashMap<String, String>>,
    /// Wait time for page load (ms)
    #[serde(rename = "waitFor", skip_serializing_if = "Option::is_none")]
    pub wait_for: Option<i32>,
    /// Whether to block ads
    #[serde(rename = "blockAds", skip_serializing_if = "Option::is_none")]
    pub block_ads: Option<bool>,
    /// Proxy mode for crawling
    #[serde(rename = "proxyMode", skip_serializing_if = "Option::is_none")]
    pub proxy_mode: Option<ProxyMode>,
    /// Whether to use AI reranking
    #[serde(rename = "useReranking", skip_serializing_if = "Option::is_none")]
    pub use_reranking: Option<bool>,
    /// Whether to enable change tracking
    #[serde(rename = "enableChangeTracking", skip_serializing_if = "Option::is_none")]
    pub enable_change_tracking: Option<bool>,
    /// Project creation timestamp
    #[serde(rename = "createdAt")]
    pub created_at: String,
    /// Last update timestamp
    #[serde(rename = "updatedAt")]
    pub updated_at: String,
    #[serde(rename = "_count", skip_serializing_if = "Option::is_none")]
    pub _count: Option<Box<models::ProjectCount>>,
}

impl Project {
    pub fn new(id: String, name: String, domain: String, user_id: String, created_at: String, updated_at: String) -> Project {
        Project {
            id,
            name,
            domain,
            user_id,
            prompt: None,
            search_query: None,
            sitemap_mode: None,
            include_subdomains: None,
            ignore_query_params: None,
            max_discovery_depth: None,
            exclude_paths: None,
            include_paths: None,
            crawl_entire_domain: None,
            allow_external_links: None,
            max_pages: None,
            crawl_delay: None,
            crawl_max_concurrency: None,
            only_main_content: None,
            custom_headers: None,
            wait_for: None,
            block_ads: None,
            proxy_mode: None,
            use_reranking: None,
            enable_change_tracking: None,
            created_at,
            updated_at,
            _count: None,
        }
    }
}
/// How to handle sitemaps
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum SitemapMode {
    #[serde(rename = "include")]
    Include,
    #[serde(rename = "exclude")]
    Exclude,
    #[serde(rename = "only")]
    Only,
}

impl Default for SitemapMode {
    fn default() -> SitemapMode {
        Self::Include
    }
}
/// Proxy mode for crawling
#[derive(Clone, Copy, Debug, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize)]
pub enum ProxyMode {
    #[serde(rename = "auto")]
    Auto,
    #[serde(rename = "residential")]
    Residential,
    #[serde(rename = "datacenter")]
    Datacenter,
}

impl Default for ProxyMode {
    fn default() -> ProxyMode {
        Self::Auto
    }
}

